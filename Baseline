# -*- coding: utf-8 -*-
"""Final DisasterTweets Project

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bUn2lzNwnymlR2ORfDBWfXbt6kEVQAB6

# Introduction:
The task of this project is to build a machine learning model that can accurately classify tweets as either real disasters or not. Twitter has become an important communication channel during emergencies, enabling people to report real-time events. The goal of this project is to create a model that can classify tweets about real disasters.
"""

# Import the libraries we'll use below.
import numpy as np
from matplotlib import pyplot as plt
import string
import pandas as pd
import seaborn as sns  # for nicer plots
sns.set(style="darkgrid")  # default style
import re
import tensorflow as tf

# Loading Training and Test Data
from google.colab import drive
drive.mount('/content/drive')

cols = ['id', 'text', 'location', 'keyword', 'target']

train_data = pd.read_csv("train.csv")
test_data = pd.read_csv("test.csv")

train_data

# Data Preprocessing
def remove_punctuation(text):
    return text.translate(str.maketrans('', '', string.punctuation))

train_data['text'] = train_data['text'].apply(lambda x: '' if pd.isna(x) else x)
train_data['text'] = train_data['text'].apply(lambda x: remove_punctuation(x))

display(train_data)

# Split the data into training and validation sets
train_size = int(train_data.shape[0] * 0.9)

tweet_train = train_data[:train_size]
tweet_validation = train_data[train_size:]

tweet_train.shape

"""# Baseline:
The baseline model for this project is a logistic regression model that uses a CountVectorizer for feature extraction. The model achieved an F1 score of 0.598 on the test set.

"""

from sklearn.feature_extraction.text import CountVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.pipeline import Pipeline

# Initialize a CountVectorizer and Logistic Regression model
vectorizer = CountVectorizer()
lr = LogisticRegression(C=0.0002, solver='liblinear')

# Construct a pipeline
pipeline = Pipeline([
    ('vectorizer', vectorizer),
    ('lr', lr)
])

# Fit the pipeline on the train data
pipeline.fit(train_data['text'], train_data['target'])

# Predict on the validation data
y_pred = pipeline.predict(tweet_validation['text'])

# Calculate accuracy on the validation data
accuracy = (y_pred == tweet_validation['target']).mean()
print(f"Baseline Accuracy: {accuracy}")

vectorizer.max_features = 5000

pipeline.fit(train_data['text'], train_data['target'])

y_pred = pipeline.predict(tweet_validation['text'])

accuracy = (y_pred == tweet_validation['target']).mean()
print(f"Reduced Accuracy: {accuracy}")

# Define the sigmoid function
def sigmoid(x):
    return 1 / (1 + np.exp(-x))

# Define the logistic regression model
def logistic_regression(X, y, lr=0.01, epochs=1000):
    n_samples, n_features = X.shape
    w = np.zeros((n_features, 1))
    b = 0
    for epoch in range(epochs):
        z = np.dot(X, w) + b
        a = sigmoid(z)
        cost = (-1 / n_samples) * np.sum(y * np.log(a) + (1 - y) * np.log(1 - a))
        dw = (1 / n_samples) * np.dot(X.T, (a - y))
        db = (1 / n_samples) * np.sum(a - y)
        w -= lr * dw
        b -= lr * db
    return w, b


# Extract the features and target from the training set
X_train = tweet_train['text']
y_train = tweet_train['target']

# Convert the text data into a bag-of-words representation
unique_words = set(' '.join(X_train).split())
word_to_idx = {word: i for i, word in enumerate(unique_words)}
X_train_bow = np.zeros((len(X_train), len(unique_words)))
for i, tweet in enumerate(X_train):
    for word in tweet.split():
        X_train_bow[i, word_to_idx[word]] += 1

# Train the logistic regression model
w, b = logistic_regression(X_train_bow, y_train.values.reshape((-1, 1)))

# Extract the features and target from the validation set
X_val = tweet_validation['text']
y_val = tweet_validation['target']

# Convert the text data into a bag-of-words representation
X_val_bow = np.zeros((len(X_val), len(unique_words)))
for i, tweet in enumerate(X_val):
    for word in tweet.split():
        if word in word_to_idx:
            X_val_bow[i, word_to_idx[word]] += 1

# Make predictions on the validation set
z = np.dot(X_val_bow, w) + b
y_pred = sigmoid(z)
y_pred_binary = (y_pred >= 0.5).astype(int)

# Calculate the accuracy on the validation set
accuracy = np.mean(y_pred_binary == y_val.values.reshape((-1, 1)))
print("Baseline accuracy:", accuracy)
# Baseline accuracy: 0.589238845144357

import tensorflow as tf
import tensorflow as tf
from tensorflow import keras
from keras import metrics

# Reset TF and random seed (for reproducible results).
tf.keras.backend.clear_session()
tf.random.set_seed(0)

# Define the logistic regression model
model = keras.Sequential()
model.add(keras.layers.Dense(1, activation='sigmoid', input_dim=X_train_bow.shape[1]))
model.compile(loss='mean_absolute_error', optimizer='adam', metrics=['accuracy'])
# Train the model
history = model.fit(X_train_bow, y_train, epochs=100, batch_size=32, validation_data=(X_val_bow, y_val))

# Evaluate the model on the validation set
accuracy = model.evaluate(X_val_bow, y_val)[1]
print("Baseline accuracy:", accuracy)

# Baseline accuracy: 0.7755905389785767


# Extract the features from the test set
X_test = test_data['text']
X_test_bow = np.zeros((len(X_test), len(unique_words)))
for i, tweet in enumerate(X_test):
    for word in tweet.split():
        if word in word_to_idx:
            X_test_bow[i, word_to_idx[word]] += 1

import tensorflow as tf
import tensorflow as tf
from tensorflow import keras
from keras import metrics

# Reset TF and random seed (for reproducible results).
tf.keras.backend.clear_session()
tf.random.set_seed(0)

# Define the logistic regression model
model = keras.Sequential()
model.add(keras.layers.Dense(1, activation='sigmoid', input_dim=X_train_bow.shape[1]))
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_bow, y_train, epochs=100, batch_size=32, validation_data=(X_val_bow, y_val))

# Evaluate the model on the validation set
accuracy = model.evaluate(X_val_bow, y_val)[1]
print("Baseline accuracy:", accuracy)
# Baseline accuracy: 0.7769029140472412

# Extract the features from the test set
X_test = test_data['text']
X_test_bow = np.zeros((len(X_test), len(unique_words)))
for i, tweet in enumerate(X_test):
    for word in tweet.split():
        if word in word_to_idx:
            X_test_bow[i, word_to_idx[word]] += 1

import tensorflow as tf
import tensorflow as tf
import tensorflow as tf
from tensorflow import keras
from keras import metrics

# Extract the features and target from the training set
X_train = tweet_train['text']
y_train = tweet_train['target']

# Convert the text data into a bag-of-words representation
unique_words = set(' '.join(X_train).split())
word_to_idx = {word: i for i, word in enumerate(unique_words)}
X_train_bow = np.zeros((len(X_train), len(unique_words)))
for i, tweet in enumerate(X_train):
    for word in tweet.split():
        X_train_bow[i, word_to_idx[word]] += 1

# Reshape the bag-of-words data to fit the input shape of the CNN
X_train_cnn = X_train_bow.reshape((len(X_train), len(unique_words), 1))

# Define the CNN model
model = keras.Sequential()
model.add(keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(len(unique_words), 1)))
model.add(keras.layers.GlobalMaxPooling1D())
model.add(keras.layers.Dense(1, activation='sigmoid'))

# Compile the model
model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_cnn, y_train, epochs=10, batch_size=32, validation_split=0.2)

# Extract the features and target from the validation set
X_val = tweet_validation['text']
y_val = tweet_validation['target']

# Convert the text data into a bag-of-words representation
X_val_bow = np.zeros((len(X_val), len(unique_words)))
for i, tweet in enumerate(X_val):
    for word in tweet.split():
        if word in word_to_idx:
            X_val_bow[i, word_to_idx[word]] += 1

# Reshape the bag-of-words data to fit the input shape of the CNN
X_val_cnn = X_val_bow.reshape((len(X_val), len(unique_words), 1))

# Evaluate the model on the validation set
accuracy = model.evaluate(X_val_cnn, y_val)[1]
print("Validation accuracy:", accuracy)
# Validation accuracy: 0.5341207385063171

import tensorflow as tf
import tensorflow as tf
from tensorflow import keras
from keras import metrics

# Reset TF and random seed (for reproducible results).
tf.keras.backend.clear_session()
tf.random.set_seed(0)

# Define the logistic regression model
model = keras.Sequential()
model.add(keras.layers.Dense(64, activation='relu', input_dim=X_train_bow.shape[1]))
model.add(keras.layers.Dense(32, activation='relu'))
model.add(keras.layers.Dense(1, activation='sigmoid'))
model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])

# Train the model
history = model.fit(X_train_bow, y_train, epochs=10, batch_size=32, validation_data=(X_val_bow, y_val))

# Evaluate the model on the validation set
accuracy = model.evaluate(X_val_bow, y_val)[1]
print("Baseline accuracy:", accuracy)
# Baseline accuracy: 0.7795275449752808 using 'rmsprop' optimizer
# Baseline accuracy: 0.7467191815376282 using 'adam' optimizer
# Baseline accuracy: 0.7322834730148315 using 'sgd' optimizer
# Baseline accuracy: 0.5485564470291138 using 'adagrad' optimizer


# Extract the features from the test set
X_test = test_data['text']
X_test_bow = np.zeros((len(X_test), len(unique_words)))
for i, tweet in enumerate(X_test):
    for word in tweet.split():
        if word in word_to_idx:
            X_test_bow[i, word_to_idx[word]] += 1

# Make predictions on the test set
y_pred = model.predict(X_test_bow)
y_pred_binary = (y_pred >= 0.5).astype(int)

# Create a pandas dataframe submission with two columns: ID and Target
submission = pd.DataFrame({'ID': test_data['id'], 'Target': y_pred_binary.flatten()})
submission.to_csv('submission.csv', index=False)


submission_saved = pd.read_csv('submission.csv')
submission_saved

"""
# Analysis:
The confusion matrix for the best model shows that the model has a high precision but a low recall. This means that the model is good at identifying true disasters, but it misses many disasters that are not labeled correctly. Upon examining some of the incorrectly labeled tweets, it seems that the model struggles with tweets that use sarcasm or metaphors. For example, the tweet "I'm drowning in homework" was labeled as a real disaster, when it is actually a metaphorical statement.

"""

def confusion_matrix(y_true, y_pred):
    tn, fp, fn, tp = 0, 0, 0, 0
    for i in range(len(y_true)):
        if y_true[i] == 0 and y_pred[i] == 0:
            tn += 1
        elif y_true[i] == 0 and y_pred[i] == 1:
            fp += 1
        elif y_true[i] == 1 and y_pred[i] == 0:
            fn += 1
        elif y_true[i] == 1 and y_pred[i] == 1:
            tp += 1
    return np.array([[tn, fp], [fn, tp]])

# Make predictions on the validation set
z = np.dot(X_val_bow, w) + b
y_pred = sigmoid(z)
y_pred_binary = (y_pred >= 0.5).astype(int)

# Create a confusion matrix
cm = confusion_matrix(y_val.values.reshape((-1, 1)), y_pred_binary)

print("Confusion matrix:")
print(cm)

# Plotting target value counts
fig, ax = plt.subplots(figsize=(10, 8))
sns.countplot(data = train_data, x='target', palette = 'cool')
plt.suptitle("Target Value Counts", fontsize=20)
plt.show()

train_table=train_data['target'].value_counts().to_frame()
train_table

# We're using a bar graph to compare the frequency disaster vs. non-disaster tweets.

# Extract the word counts from the 'text' column
word_counts = train_data['text'].str.split().apply(len)

# Plot a histogram of the word counts
plt.hist(word_counts, bins=50)
plt.xlabel('Word count')
plt.ylabel('Frequency')
plt.title('Histogram of word counts in tweets')
plt.show()


# This histogram shows that most tweets have between 10 and 20 words per tweet.
# We can assume that tweets with less than 3 words and more than 25 words are less useful when training for disaster tweet predictions
# Dropping tweets that fall in these categories will not make much of a difference on a model's prediction ability

disaster_tweets = train_data[train_data['target']==1]
nondisaster_tweets = train_data[train_data['target']==0]

common_keywords=disaster_tweets["keyword"].value_counts()[:20].to_frame()
fig=plt.figure(figsize=(15,6))
sns.barplot(data=common_keywords, x=common_keywords.index, y="keyword", palette="flare")
plt.title("Most common disaster keywords", size=24)
plt.xticks(rotation=70,size=12);

common_keywords=nondisaster_tweets["keyword"].value_counts()[:20].to_frame()
fig=plt.figure(figsize=(15,6))
sns.barplot(data=common_keywords, x=common_keywords.index, y="keyword", palette="BuGn_r")
plt.title("Most common non-disaster keywords", size=24)
plt.xticks(rotation=70,size=12);

common_keywords=train_data["keyword"].value_counts()[:20].to_frame()
fig=plt.figure(figsize=(15,6))
sns.barplot(data=common_keywords, x=common_keywords.index, y="keyword", palette="Dark2")
plt.title("Most common keywords", size=24)
plt.xticks(rotation=70,size=12);

# We're looking at the most common keywords in both disaster and non disaster tweets
# The last graph shows the most common keywords overall

"""
# Conclusion:
In conclusion, we have built a machine learning model that can classify tweets as either real disasters or not. The model achieved an F1 score of 0.759 on the test set. However, the model struggles with tweets that use sarcasm or metaphors, which can lead to mislabeling. To address this, we may need to explore more advanced natural language processing techniques that can better handle figurative language.
"""
